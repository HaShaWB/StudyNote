---
tags:
  - paper
  - AI
---
- Attention : 데이터를 Key-Value-Querry로 나눠 연산
- Positional Encoding : Sequence 상에서 데이터의 위치를 절대적으로 표현
![[Attention is All You Need Full.png]]


![[Positional Encoding#Basic Positional Encoding|Positional Encoding]]


![[Attention#Scaled Dot-Product Attention]]

![[Attention#Multi-Head Attention]]

## Transformers
- Multi-head Attention Layer를 활용한 모델